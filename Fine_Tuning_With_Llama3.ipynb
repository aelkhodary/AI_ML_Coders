{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNg6JHw6RP4si9Dx374iIGz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aelkhodary/AI_ML_Coders/blob/main/Fine_Tuning_With_Llama3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-ZZrxdW-G43g"
      },
      "outputs": [],
      "source": [
        "!pip install torchao -q\n",
        "!pip3 install torchtune -q\n",
        "# Install the datasets library\n",
        "!pip install datasets -q\n",
        "# Install the huggingface_hub library\n",
        "!pip install huggingface_hub -q\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PATH'] += ':/usr/local/bin'"
      ],
      "metadata": {
        "id": "H7f5iQKSJ5GQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!which tune\n",
        "!tune ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCYr0t0EJSpB",
        "outputId": "53fd63b8-c07b-4394-fbe2-7976deb67373"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/tune\n",
            "RECIPE                                   CONFIG                                  \n",
            "full_finetune_single_device              llama2/7B_full_low_memory               \n",
            "                                         code_llama2/7B_full_low_memory          \n",
            "                                         llama3/8B_full_single_device            \n",
            "                                         llama3_1/8B_full_single_device          \n",
            "                                         llama3_2/1B_full_single_device          \n",
            "                                         llama3_2/3B_full_single_device          \n",
            "                                         mistral/7B_full_low_memory              \n",
            "                                         phi3/mini_full_low_memory               \n",
            "                                         qwen2/7B_full_single_device             \n",
            "                                         qwen2/0.5B_full_single_device           \n",
            "                                         qwen2/1.5B_full_single_device           \n",
            "                                         qwen2_5/0.5B_full_single_device         \n",
            "                                         qwen2_5/1.5B_full_single_device         \n",
            "                                         qwen2_5/3B_full_single_device           \n",
            "                                         qwen2_5/7B_full_single_device           \n",
            "                                         llama3_2_vision/11B_full_single_device  \n",
            "full_finetune_distributed                llama2/7B_full                          \n",
            "                                         llama2/13B_full                         \n",
            "                                         llama3/8B_full                          \n",
            "                                         llama3_1/8B_full                        \n",
            "                                         llama3_2/1B_full                        \n",
            "                                         llama3_2/3B_full                        \n",
            "                                         llama3/70B_full                         \n",
            "                                         llama3_1/70B_full                       \n",
            "                                         llama3_3/70B_full                       \n",
            "                                         mistral/7B_full                         \n",
            "                                         gemma/2B_full                           \n",
            "                                         gemma/7B_full                           \n",
            "                                         gemma2/2B_full                          \n",
            "                                         gemma2/9B_full                          \n",
            "                                         gemma2/27B_full                         \n",
            "                                         phi3/mini_full                          \n",
            "                                         qwen2/7B_full                           \n",
            "                                         qwen2/0.5B_full                         \n",
            "                                         qwen2/1.5B_full                         \n",
            "                                         qwen2_5/0.5B_full                       \n",
            "                                         qwen2_5/1.5B_full                       \n",
            "                                         qwen2_5/3B_full                         \n",
            "                                         qwen2_5/7B_full                         \n",
            "                                         llama3_2_vision/11B_full                \n",
            "                                         llama3_2_vision/90B_full                \n",
            "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
            "                                         llama2/7B_qlora_single_device           \n",
            "                                         code_llama2/7B_lora_single_device       \n",
            "                                         code_llama2/7B_qlora_single_device      \n",
            "                                         llama3/8B_lora_single_device            \n",
            "                                         llama3_1/8B_lora_single_device          \n",
            "                                         llama3/8B_qlora_single_device           \n",
            "                                         llama3_2/1B_lora_single_device          \n",
            "                                         llama3_2/3B_lora_single_device          \n",
            "                                         llama3/8B_dora_single_device            \n",
            "                                         llama3/8B_qdora_single_device           \n",
            "                                         llama3_1/8B_qlora_single_device         \n",
            "                                         llama3_2/1B_qlora_single_device         \n",
            "                                         llama3_2/3B_qlora_single_device         \n",
            "                                         llama2/13B_qlora_single_device          \n",
            "                                         mistral/7B_lora_single_device           \n",
            "                                         mistral/7B_qlora_single_device          \n",
            "                                         gemma/2B_lora_single_device             \n",
            "                                         gemma/2B_qlora_single_device            \n",
            "                                         gemma/7B_lora_single_device             \n",
            "                                         gemma/7B_qlora_single_device            \n",
            "                                         gemma2/2B_lora_single_device            \n",
            "                                         gemma2/2B_qlora_single_device           \n",
            "                                         gemma2/9B_lora_single_device            \n",
            "                                         gemma2/9B_qlora_single_device           \n",
            "                                         gemma2/27B_lora_single_device           \n",
            "                                         gemma2/27B_qlora_single_device          \n",
            "                                         phi3/mini_lora_single_device            \n",
            "                                         phi3/mini_qlora_single_device           \n",
            "                                         qwen2/7B_lora_single_device             \n",
            "                                         qwen2/0.5B_lora_single_device           \n",
            "                                         qwen2/1.5B_lora_single_device           \n",
            "                                         qwen2_5/0.5B_lora_single_device         \n",
            "                                         qwen2_5/1.5B_lora_single_device         \n",
            "                                         qwen2_5/3B_lora_single_device           \n",
            "                                         qwen2_5/7B_lora_single_device           \n",
            "                                         qwen2_5/14B_lora_single_device          \n",
            "                                         llama3_2_vision/11B_lora_single_device  \n",
            "                                         llama3_2_vision/11B_qlora_single_device \n",
            "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
            "                                         llama3_1/8B_lora_dpo_single_device      \n",
            "lora_dpo_distributed                     llama2/7B_lora_dpo                      \n",
            "                                         llama3_1/8B_lora_dpo                    \n",
            "ppo_full_finetune_single_device          mistral/7B_full_ppo_low_memory          \n",
            "lora_finetune_distributed                llama2/7B_lora                          \n",
            "                                         llama2/13B_lora                         \n",
            "                                         llama2/70B_lora                         \n",
            "                                         llama2/7B_qlora                         \n",
            "                                         llama2/70B_qlora                        \n",
            "                                         llama3/8B_dora                          \n",
            "                                         llama3/70B_lora                         \n",
            "                                         llama3_1/70B_lora                       \n",
            "                                         llama3_3/70B_lora                       \n",
            "                                         llama3_3/70B_qlora                      \n",
            "                                         llama3/8B_lora                          \n",
            "                                         llama3_1/8B_lora                        \n",
            "                                         llama3_2/1B_lora                        \n",
            "                                         llama3_2/3B_lora                        \n",
            "                                         llama3_1/405B_qlora                     \n",
            "                                         mistral/7B_lora                         \n",
            "                                         gemma/2B_lora                           \n",
            "                                         gemma/7B_lora                           \n",
            "                                         gemma2/2B_lora                          \n",
            "                                         gemma2/9B_lora                          \n",
            "                                         gemma2/27B_lora                         \n",
            "                                         phi3/mini_lora                          \n",
            "                                         qwen2/7B_lora                           \n",
            "                                         qwen2/0.5B_lora                         \n",
            "                                         qwen2/1.5B_lora                         \n",
            "                                         qwen2_5/0.5B_lora                       \n",
            "                                         qwen2_5/1.5B_lora                       \n",
            "                                         qwen2_5/3B_lora                         \n",
            "                                         qwen2_5/7B_lora                         \n",
            "                                         qwen2_5/32B_lora                        \n",
            "                                         qwen2_5/72B_lora                        \n",
            "                                         llama3_2_vision/11B_lora                \n",
            "                                         llama3_2_vision/11B_qlora               \n",
            "                                         llama3_2_vision/90B_lora                \n",
            "                                         llama3_2_vision/90B_qlora               \n",
            "generate                                 generation                              \n",
            "dev/generate_v2                          llama2/generation_v2                    \n",
            "                                         llama3_2_vision/11B_generation_v2       \n",
            "dev/early_exit_finetune_distributed      llama2/7B_full_early_exit               \n",
            "eleuther_eval                            eleuther_evaluation                     \n",
            "                                         llama3_2_vision/11B_evaluation          \n",
            "                                         qwen2/evaluation                        \n",
            "                                         gemma/evaluation                        \n",
            "                                         phi3/evaluation                         \n",
            "                                         mistral/evaluation                      \n",
            "quantize                                 quantization                            \n",
            "qat_distributed                          llama2/7B_qat_full                      \n",
            "                                         llama3/8B_qat_full                      \n",
            "qat_lora_finetune_distributed            llama3/8B_qat_lora                      \n",
            "                                         llama3_1/8B_qat_lora                    \n",
            "                                         llama3_2/1B_qat_lora                    \n",
            "                                         llama3_2/3B_qat_lora                    \n",
            "knowledge_distillation_single_device     qwen2/1.5_to_0.5B_KD_lora_single_device \n",
            "                                         llama3_2/8B_to_1B_KD_lora_single_device \n",
            "knowledge_distillation_distributed       qwen2/1.5_to_0.5B_KD_lora_distributed   \n",
            "                                         llama3_2/8B_to_1B_KD_lora_distributed   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "Listing TorchTune recipes\n",
        "You are working on a text classification project to fine-tune a 1 billion parameter Llama 3.2 model. Given your hardware constraints, you need to choose a single device configuration, with full fine-tuning. To list all available options, you can use the console and run !tune ls.\n",
        "\n",
        "Which recipe would be compatible with your model based on the constraints?\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "Possible answers\n",
        "\n",
        "\n",
        "\n",
        "full_finetune_distributed with the llama3_2/1B_full configuration\n",
        "\n",
        "full_finetune_single_device with the llama3_1/8B_full_single_device configuration\n",
        "\n",
        ">>> full_finetune_single_device with the llama3_2/1B_full_single_device configuration"
      ],
      "metadata": {
        "id": "g0LkUDqWNcgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running a TorchTune task**:\n",
        "\n",
        "Having listed your choices with !tune ls, you are now ready to launch your fine-tuning task with the recipe and configuration for your Llama 3.2, 1B model on single device (full_finetune_single_device with llama3_2/1B_full_single_device).\n",
        "\n",
        "Which of the following commands will you use to run the task for 20 epochs, on a single device GPU?\n",
        "\n",
        "\n",
        "**Possible Answers**\n",
        "\n",
        "\n",
        "!torchtune run full_finetune_single_device --config llama3_2/1B_full_single_device device=cpu epochs=0\n",
        "\n",
        "\n",
        ">> !tune run full_finetune_single_device --config llama3_2/1B_full_single_device device=cuda epochs=20\n",
        "\n",
        "\n",
        "!tune run full_finetune_single_device --config llama3_2/1B_full_single_device device=gpu epochs=20"
      ],
      "metadata": {
        "id": "0DT5CuCeO5i-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "Filtering datasets for evaluation\n",
        "You are building a training and evaluation pipeline for your company's health care chatbot, which is used by hospitals to onboard new patients.\n",
        "\n",
        "Your task is to create a pipeline to load the MedQuad-MedicalQnADataset to evaluate an LLM on its ability to answer medical questions. You are asked to load the dataset in the ds variable, and only include the first 500 samples of the train split of the dataset stored in dataset_name as your evaluation set.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "Import necessary functions and classes from datasets.\n",
        "Load the dataset in the ds variable.\n",
        "Manipulate ds to include the first 500 samples of the train split of the dataset stored in dataset_name as your evaluation set."
      ],
      "metadata": {
        "id": "Yxr4LojfjFt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load neccesary imports from library\n",
        "from datasets import load_dataset, Dataset\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from pprint import pprint\n",
        "\n",
        "# Retrieve the Hugging Face API key from Colab Secrets\n",
        "hf_api_key = userdata.get('HF_TOKEN')\n",
        "# Log in to Hugging Face\n",
        "login(token=hf_api_key)\n",
        "\n",
        "dataset_name = \"keivalya/MedQuad-MedicalQnADataset\"\n",
        "# Load the training split of the dataset\n",
        "ds = load_dataset(dataset_name, split='train')\n",
        "\n",
        "# Filter for the first 500 samples of the dataset\n",
        "filtered_ds = Dataset.from_dict(ds[:500])\n",
        "print(filtered_ds.shape)\n",
        "print(filtered_ds.column_names)\n",
        "pprint(filtered_ds[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md8wauy0Ns4S",
        "outputId": "7c73bf8f-12a9-4b3e-9eca-a22381b2a65c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 3)\n",
            "['qtype', 'Question', 'Answer']\n",
            "{'Answer': 'LCMV infections can occur after exposure to fresh urine, '\n",
            "           'droppings, saliva, or nesting materials from infected rodents.  '\n",
            "           'Transmission may also occur when these materials are directly '\n",
            "           'introduced into broken skin, the nose, the eyes, or the mouth, or '\n",
            "           'presumably, via the bite of an infected rodent. Person-to-person '\n",
            "           'transmission has not been reported, with the exception of vertical '\n",
            "           'transmission from infected mother to fetus, and rarely, through '\n",
            "           'organ transplantation.',\n",
            " 'Question': 'Who is at risk for Lymphocytic Choriomeningitis (LCM)? ?',\n",
            " 'qtype': 'susceptibility'}\n"
          ]
        }
      ]
    }
  ]
}