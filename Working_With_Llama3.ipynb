{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOduf3bLl76K0cZQH69kgKz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "869b38abefec4d2284172d19e21a978b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aaa5a024c7dd4e8e93f5d45619d6ba41",
              "IPY_MODEL_3eb9efc1eb36481b83a54018d95fa09b",
              "IPY_MODEL_d8f5b43ec45e452d989f677dac4dbe00"
            ],
            "layout": "IPY_MODEL_fcad50a2bcbe4dd8a01f09c898a32139"
          }
        },
        "aaa5a024c7dd4e8e93f5d45619d6ba41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66fb038f78e844a48a8cb679d8f578d5",
            "placeholder": "​",
            "style": "IPY_MODEL_44d05fad6e01496c9c9ee5dc35df1817",
            "value": "Llama-3.2-1B-Instruct-Q3_K_L.gguf: 100%"
          }
        },
        "3eb9efc1eb36481b83a54018d95fa09b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6adc33b5d7049e98ed72ba7438157f2",
            "max": 732524384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6c399edf1b646138e6293b34c6981c1",
            "value": 732524384
          }
        },
        "d8f5b43ec45e452d989f677dac4dbe00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c02f4d5d24a48fd99df9ef9c3bbb284",
            "placeholder": "​",
            "style": "IPY_MODEL_09df27d463ea499695c87c3e54ec1397",
            "value": " 733M/733M [00:06&lt;00:00, 134MB/s]"
          }
        },
        "fcad50a2bcbe4dd8a01f09c898a32139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66fb038f78e844a48a8cb679d8f578d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44d05fad6e01496c9c9ee5dc35df1817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6adc33b5d7049e98ed72ba7438157f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6c399edf1b646138e6293b34c6981c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c02f4d5d24a48fd99df9ef9c3bbb284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09df27d463ea499695c87c3e54ec1397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aelkhodary/AI_ML_Coders/blob/main/Working_With_Llama3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Link to visit\n",
        "https://llama-cpp-python.readthedocs.io/en/latest/\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "T9L-w5GgCPL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Quantized models and the GGUF format are techniques and file formats used to optimize large language models (LLMs) for efficient inference, especially on devices with limited computational resources (e.g., CPUs or low-memory GPUs). Let me break it down for you:\n",
        "\n",
        "1. What is Quantization?\n",
        "Quantization is a process of reducing the precision of the numbers used to represent a model's weights and activations. In simpler terms, it compresses the model by using fewer bits to store each number, which reduces the model's size and speeds up inference.\n",
        "\n",
        "Why Quantize?\n",
        "Smaller Model Size: Quantized models take up less disk space and memory.\n",
        "\n",
        "Faster Inference: Lower precision calculations are faster, especially on CPUs.\n",
        "\n",
        "Lower Hardware Requirements: Quantized models can run on devices with limited resources (e.g., laptops, edge devices).\n",
        "\n",
        "Common Quantization Levels:\n",
        "FP32 (32-bit floating point): Original precision, no quantization.\n",
        "\n",
        "FP16 (16-bit floating point): Half precision, smaller and faster.\n",
        "\n",
        "INT8 (8-bit integer): Even smaller and faster, with some loss of accuracy.\n",
        "\n",
        "INT4 (4-bit integer): Highly compressed, fastest, but with more accuracy loss.\n",
        "\n",
        "For example:\n",
        "\n",
        "A model originally in FP32 might be quantized to INT8 or INT4, reducing its size by 4x or 8x, respectively.\n",
        "\n",
        "2. What is GGUF?\n",
        "GGUF (GPT-Generated Unified Format) is a file format introduced by the llama.cpp project to store quantized models efficiently.\n",
        "It is the successor to the older GGML format and is designed to be more flexible and future-proof.\n",
        "\n",
        "Key Features of GGUF:\n",
        "Supports Multiple Quantization Levels: GGUF files can store models quantized to different levels (e.g., Q4_K_M, Q5_K_M, Q8_0).\n",
        "\n",
        "Optimized for CPU Inference: GGUF models are designed to run efficiently on CPUs, making them ideal for devices without GPUs.\n",
        "\n",
        "Self-Contained: GGUF files include all necessary metadata (e.g., model architecture, tokenizer configuration) in a single file.\n",
        "\n",
        "Cross-Platform: GGUF models can be used across different operating systems (Linux, Windows, macOS).\n",
        "\n",
        "Why Use GGUF?\n",
        "Efficiency: GGUF models are highly optimized for inference, making them ideal for resource-constrained environments.\n",
        "\n",
        "Ease of Use: A single .gguf file contains everything needed to run the model.\n",
        "\n",
        "Compatibility: GGUF is widely supported by tools like llama.cpp and llama-cpp-python.\n",
        "\n",
        "3. How Quantization and GGUF Work Together\n",
        "A model is first trained in high precision (e.g., FP32 or FP16).\n",
        "\n",
        "After training, the model is quantized to a lower precision (e.g., INT8 or INT4) and saved in GGUF format.\n",
        "\n",
        "The quantized GGUF model can then be loaded and run efficiently using tools like llama.cpp or llama-cpp-python.\n",
        "\n",
        "4. Example of Quantized GGUF Models\n",
        "When you browse a model repository on Hugging Face (e.g., TheBloke/Llama-2-7B-GGUF), you’ll see files like:\n",
        "\n",
        "llama-2-7b.Q4_K_M.gguf\n",
        "\n",
        "llama-2-7b.Q5_K_M.gguf\n",
        "\n",
        "llama-2-7b.Q8_0.gguf\n",
        "\n",
        "These files represent the same model but quantized to different levels:\n",
        "\n",
        "Q4_K_M: 4-bit quantization, medium precision.\n",
        "\n",
        "Q5_K_M: 5-bit quantization, medium precision.\n",
        "\n",
        "Q8_0: 8-bit quantization, higher precision.\n",
        "\n",
        "5. Trade-offs of Quantization\n",
        "While quantization offers significant benefits, it also comes with some trade-offs:\n",
        "\n",
        "Accuracy Loss: Lower precision can reduce the model's accuracy, especially for complex tasks.\n",
        "\n",
        "Quantization Artifacts: Some models may behave differently after quantization.\n",
        "\n",
        "Not Suitable for Training: Quantization is primarily used for inference; training requires higher precision.\n",
        "\n",
        "6. How to Use GGUF Models\n",
        "To use a GGUF model, you typically:\n",
        "\n",
        "Download the .gguf file from a repository like Hugging Face.\n",
        "\n",
        "Load the model using a compatible library (e.g., llama-cpp-python).\n",
        "\n",
        "Run inference on the model.\n",
        "\n",
        "Example Code:\n",
        "python\n",
        "Copy\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the GGUF model\n",
        "model_path = \"llama-2-7b.Q4_K_M.gguf\"\n",
        "llm = Llama(model_path=model_path)\n",
        "\n",
        "# Run inference\n",
        "prompt = \"What is the capital of France?\"\n",
        "output = llm(prompt)\n",
        "print(output['choices'][0]['text'])\n",
        "\n",
        "\n",
        "7. Comparison: GGUF vs PyTorch Models\n",
        "Feature\t-->GGUF Format\t-->PyTorch Format (e.g., .bin)\n",
        "File Size\t-->Smaller (due to quantization)\tLarger (full precision)\n",
        "Inference Speed\t-->Faster (optimized for CPU)\tSlower (requires GPU for best speed)\n",
        "Hardware Support\t-->Runs on CPUs and low-resource devices\tRequires GPUs for efficient inference\n",
        "Use Case\t-->Inference-only\tTraining and inference\n",
        "Flexibility\t-->Limited to inference\tFull flexibility for training and fine-tuning\n",
        "\n",
        "\n",
        "Summary\n",
        "Quantization reduces the precision of a model's weights to make it smaller and faster.\n",
        "\n",
        "GGUF is a file format designed to store quantized models efficiently, optimized for CPU inference.\n",
        "\n",
        "Quantized GGUF models are ideal for running large language models on devices with limited resources.\n",
        "\n",
        "Let me know if you need further clarification!\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ofat9N4B8RWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD3CMXGA8JhL"
      },
      "outputs": [],
      "source": [
        "# Install Models file\n",
        "\"\"\"\n",
        "Pulling models from Hugging Face Hub\n",
        "You can download Llama models in gguf format directly from Hugging Face using the from_pretrained method.\n",
        "You'll need to install the huggingface-hub package to use this feature (pip install huggingface-hub).\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n",
        "    filename=\"*q8_0.gguf\",\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "By default from_pretrained will download the model to the huggingface cache directory, you can then manage installed model files with the huggingface-cli tool.\n",
        "\n",
        "\n",
        "1. What is filename?\n",
        "The filename is the name of the specific file you want to download from the repository.\n",
        "\n",
        "In the case of GGUF models, this is typically a .gguf file that contains the quantized version of the model.\n",
        "\n",
        "You must provide the exact name of the file as it appears in the repository. Wildcards (e.g., *) are not supported.\n",
        "\n",
        "2. Where to Find the filename?\n",
        "Go to the model's Hugging Face repository page (e.g., meta-llama/Llama-3.2-1B).\n",
        "\n",
        "Navigate to the \"Files and versions\" tab.\n",
        "\n",
        "Look for the .gguf file you want to download. For example, you might see files like:\n",
        "\n",
        "llama-3.2-1b.Q4_K_M.gguf\n",
        "\n",
        "llama-3.2-1b.Q5_K_M.gguf\n",
        "\n",
        "llama-3.2-1b.Q8_0.gguf\n",
        "\n",
        "Copy the exact name of the file you want to download and use it as the filename argument.\n",
        "\n",
        "\n",
        "3. Why is filename Important?\n",
        "Hugging Face repositories can contain multiple files (e.g., model weights, configuration files, tokenizer files, etc.).\n",
        "\n",
        "The filename ensures that you download the specific file you need (in this case, the .gguf file).\n",
        "\n",
        "If you provide an incorrect or non-existent filename, the download will fail with an error.\n",
        "\n",
        "\n",
        "######## Load model using transfoemer :\n",
        "The code you provided mixes two different libraries and approaches:\n",
        "\n",
        "transformers library: Used for loading models like meta-llama/Llama-3.2-1B in PyTorch format.\n",
        "\n",
        "llama-cpp-python library: Used for loading GGUF models (quantized models compatible with llama.cpp).\n",
        "\n",
        "These two libraries are not directly compatible because:\n",
        "\n",
        "transformers loads models in PyTorch format (e.g., .bin files).\n",
        "\n",
        "llama-cpp-python loads models in GGUF format (e.g., .gguf files).\n",
        "\n",
        "If you want to use the llama-cpp-python library, you need to download a GGUF version of the model, not the PyTorch version. Here's how you can modify your code:\n",
        "\n",
        "#Call another model\n",
        "\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "\n",
        "\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "model = Llama(model_path=model_path)\n",
        "# Run inference\n",
        "prompt = \"What is the capital of France?\"\n",
        "prompt = \"What is the capital of Egypt?\"\n",
        "output = llm(prompt)\n",
        "print(output['choices'][0]['text'])\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip install huggingface-hub --quiet\n",
        "\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Replace with your Hugging Face token\n",
        "#login(token=\"YOUR_HUG_FACE_TOKEN\")\n",
        "# hf_hub_download(\n",
        "#     repo_id=\"meta-llama/Llama-3.2-1B\",\n",
        "#     filename=\"llama-3.2-1b.Q4_K_M.gguf\",\n",
        "#     cache_dir=\"/content/model\"\n",
        "# )\n",
        "\n",
        "\n",
        "model_path =hf_hub_download(\n",
        "    repo_id=\"medmekk/Llama-3.2-1B-Instruct.GGUF\",\n",
        "    filename=\"Llama-3.2-1B-Instruct-Q3_K_L.gguf\",\n",
        "    cache_dir=\"/content/model\"\n",
        ")\n",
        "\n",
        "print(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "869b38abefec4d2284172d19e21a978b",
            "aaa5a024c7dd4e8e93f5d45619d6ba41",
            "3eb9efc1eb36481b83a54018d95fa09b",
            "d8f5b43ec45e452d989f677dac4dbe00",
            "fcad50a2bcbe4dd8a01f09c898a32139",
            "66fb038f78e844a48a8cb679d8f578d5",
            "44d05fad6e01496c9c9ee5dc35df1817",
            "c6adc33b5d7049e98ed72ba7438157f2",
            "d6c399edf1b646138e6293b34c6981c1",
            "6c02f4d5d24a48fd99df9ef9c3bbb284",
            "09df27d463ea499695c87c3e54ec1397"
          ]
        },
        "id": "zSsQnA0V9Sbh",
        "outputId": "37abf2d0-5d29-481e-f2ba-3d8eb033b34b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Llama-3.2-1B-Instruct-Q3_K_L.gguf:   0%|          | 0.00/733M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "869b38abefec4d2284172d19e21a978b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/model/models--medmekk--Llama-3.2-1B-Instruct.GGUF/snapshots/52fc2aa2f47dd03c7f666680eb080240b4fd6c96/Llama-3.2-1B-Instruct-Q3_K_L.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python --quiet\n",
        "from llama_cpp import Llama\n",
        "\n",
        "#llm = Llama(model_path=model_path, stop=[\"Q:\", \"\\n\"])\n",
        "# Initialize the Llama model with a larger context window\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=2048,  # Increase context window to 2048 tokens\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Run inference\n",
        "prompt = \"What is the capital of France?\"\n",
        "prompt = \"What is the capital of Egypt?\"\n",
        "output = llm(prompt)\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE71OBSYNiv2",
        "outputId": "48bb7379-89ee-425f-f313-df2bdc807af7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Cairo.\n",
            "\n",
            "Cairo is the largest city in Egypt and the country's economic and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing Llama 3 completion outputs :\n",
        "\n",
        "Your company wants to use the Llama models in its Bronx Zoo question-answering bot for the animal exhibits.\n",
        "\n",
        "Your task is to extract the model's completion from the result stored in output. The output contains the completion and many other metadata. An early step to evaluate the model is to ask Llama 3 a question, and figure out how to parse its output. You are given a Llama model preloaded in llm, and given the prompt which asks it to name five foods that llamas eat, with the result stored in output.\n",
        "\n",
        "You are tasked with parsing the result in output and only retrieve the string result of the completion and store it in completion_string.\n",
        "\n",
        "\n",
        "Instructions :\n",
        "\n",
        "\n",
        "Limit the number of tokens generated to a max of 20 tokens.\n",
        "Stop the generation if the completion produces a line break, ie '\\n'.\n",
        "Parse the output variable and store the completion string in a new variable, completion_string.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "      model_path=\"./models/7B/llama-model.gguf\",\n",
        "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
        "      # seed=1337, # Uncomment to set a specific seed to make sure you will\n",
        "                     get the same result every time\n",
        "      # n_ctx=2048, # Uncomment to increase the context window\n",
        ")\n",
        "output = llm(\n",
        "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
        "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=True # Echo the prompt back in the output\n",
        ") # Generate a completion, can also call create_completion\n",
        "print(output)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "M6Bd6ZwoDsDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "\t\"Q: Name 5 foods that llamas eat? A: \", # prompt\n",
        "  \t# restrict to 20 tokens\n",
        "\tmax_tokens=32,\n",
        "\t# add relevant stopping tokens\n",
        "\tstop=[\"Q:\", \"\\n\"],\n",
        "  # to get same result every time\n",
        "  seed=1337,\n",
        "  temperature=0.8,#default value\n",
        " repeat_penalty=1.1#default value(make the model produce new words by reducing the probabilities of words that it has already produced)\n",
        ")\n",
        "# Retrieve the completion text and store in completion_string\n",
        "completion_string = output['choices'][0]['text']\n",
        "print(completion_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyLOLTWmDpda",
        "outputId": "066a88f8-130c-4148-a8ba-b6026e6ee9af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Almonds, 2. Carrots, 3. Apples, 4. Sunflower seeds, 5. Hay.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "use chat completions makes the llm more conversational.\n",
        "\"\"\"\n",
        "\n",
        "output = llm.create_chat_completion(\n",
        "      messages = [\n",
        "          {\"role\": \"system\",\n",
        "           \"content\": \"You are an assistant who perfectly describes the animals in the zoo.\"},\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": \"Name 5 foods that llamas eat?\"\n",
        "          }\n",
        "      ]\n",
        ")\n",
        "# Retrieve the completion text and store in completion_string\n",
        "completion_string = output['choices'][0]['message']\n",
        "print (completion_string)\n",
        "\n",
        "print (\"***********************************************\")\n",
        "\n",
        "\"\"\"\n",
        " Same logic after adding max_tokens, stop, seed, temperature, repeat_penalty\n",
        "\"\"\"\n",
        "output = llm.create_chat_completion(\n",
        "      messages = [\n",
        "          {\"role\": \"system\",\n",
        "           \"content\": \"You are an assistant who perfectly describes the animals in the zoo.\"},\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": \"Name 5 foods that llamas eat?\"\n",
        "          }\n",
        "      ],\n",
        "      \tmax_tokens=500,\n",
        "\t# add relevant stopping tokens\n",
        "\t#stop=[\"Q:\", \"\\n\"],\n",
        "  # to get same result every time\n",
        "  seed=1337,\n",
        "  temperature=0.8,#default value\n",
        " repeat_penalty=1.1\n",
        ")\n",
        "# Retrieve the completion text and store in completion_string\n",
        "completion_string = output['choices'][0]['message'][\"content\"]\n",
        "print (completion_string)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAbX6BJEJzM7",
        "outputId": "9f069a8e-91ef-4fe9-d49b-3f4cc85bab6a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'role': 'assistant', 'content': \"As a knowledgeable assistant, I'd be happy to help. Llamas are herbivores, which means they primarily eat plants. Here are 5 foods that llamas typically enjoy:\\n\\n1. Grass: Llamas love to graze on various types of grass, including timothy grass, orchard grass, and bluegrass.\\n2. Hay: Hay is a staple in a llama's diet, and they often eat it in large quantities. Timothy hay is a popular choice due to its high nutritional value.\\n3. Alfalfa: Alfalfa is a legume that's rich in nutrients, making it a nutritious addition to a llama's diet. It's often fed to them in small amounts.\\n4. Silage: Silage is a fermented plant-based food made from grasses, legumes, or other crops. It's a nutritious and energy-rich food source for llamas.\\n5. Alfalfa hay and silage: Many llamas are fed a combination of alfalfa hay and silage, which provides a balanced mix of nutrients and fiber.\\n\\nRemember, it's essential to provide a balanced diet for llamas, and their dietary needs may vary depending on factors like age, breed, and climate.\"}\n",
            "***********************************************\n",
            "As a knowledgeable assistant, I'd be happy to help. Llamas are herbivores, which means they primarily eat plants. Here are 5 foods that llamas typically consume:\n",
            "\n",
            "1. Grasses: Llamas love to graze on various types of grasses, including timothy grass and orchard grass.\n",
            "2. Leaves: They also enjoy munching on leaves from trees and shrubs, such as apple, cherry, and pine trees.\n",
            "3. Fruits: Llamas will occasionally eat fruits like apples, pears, and berries.\n",
            "4. Hay: Hay is an essential part of a llama's diet, especially in the summer months when grasses may not be available.\n",
            "5. Alfalfa hay: Alfalfa hay, which is high in nitrogen, is particularly popular among llamas due to its nutritional value.\n",
            "\n",
            "It's worth noting that a well-balanced diet for llamas should include a mix of these foods and also consider their specific nutritional needs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "**More creative Llama completions**\n",
        "\n",
        "You are a software developer working on integrating Llama in your company's chatbot pipelines. Unfortunately, the current Llama model you are using produces repetitive completions and often produces exactly the same results if you ask it the same question, which makes the bot feel less personable to your users.\n",
        "\n",
        "You decide to debug this issue by looking through the completion code and modify it so that the responses produced are more varied. The model is already instantiated with a model using llama_cpp and is stored in llm.\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "Add the parameter and a corresponding value to the completion code so the model considers a wider variety of words during generation.\n",
        "\n",
        "Add the parameter to the completion code which penalizes the model for repeating the same words often."
      ],
      "metadata": {
        "id": "Gx2wOOeEWveM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "\t\t\"Q: Give me directions from grand central station to the Empire State building. A: \",\n",
        "  \t\t# Modify for the model to sample from more words\n",
        "\t\ttemperature=2,\n",
        "  \t\t# Modify to penalize repeated use of the same words\n",
        "\t\trepeat_penalty=2,\n",
        "        max_tokens=15,\n",
        "        stop=[\"Q:\", \"\\n\"],\n",
        "        echo=False\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['text'])\n",
        "#1) Exit of 34th Street at Broadway (toward the west\n",
        "#1) Walk east on 33rd Street for five blocks, then turn\n",
        "#1) Head north on Broadway. 2 or a (c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTm7CCWfW6ch",
        "outputId": "6a040e1e-1830-45e1-9b0d-3c93fe28be31"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 18 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     149.61 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =     410.14 ms /    14 runs   (   29.30 ms per token,    34.13 tokens per second)\n",
            "llama_perf_context_print:       total time =     451.05 ms /    15 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Head north on Broadway. 2 or a (c)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "**Make a philosophy chatbot**\n",
        "You are a tester at a company building AI personas, and your task is to evaluate how well the new Llama models are able to generate completions in certain voices and styles.\n",
        "\n",
        "You will make a chatbot that thinks it's a philosopher and answers questions by pretending it is Plato. You are given a partially completed create_chat_completion call, which you will modify to make the chatbot respond to a user's question as if it was Plato himself.\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "\n",
        "Fill in the dictionary in the first index of the messages list with the instructions to make the model respond as if it is the Greek philosopher Plato and the appropriate role.\n",
        "Fill in the dictionary in the second index of messages with the prompting question from the user and the appropriate role.\n",
        "Ensure that both your instruction, and the user's question are correctly passed to the function call."
      ],
      "metadata": {
        "id": "JVDvgY2waPUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = [\n",
        "\t# Instruct the model to behave like Plato\n",
        "\t{\"role\": \"system\",\n",
        "\t \"content\": \"You are the Greek philosopher Plato. Answer every question using his voice.\"\n",
        "\t},\n",
        "\t# Identify that the following text is from the user\n",
        "\t{\n",
        "      \"role\": \"user\",\n",
        "\t\t\t\"content\": \"Can any shape that exist in the real world be perfect and why?\"\n",
        "    }\n",
        "]\n",
        "# Pass in conversation context to the completion call\n",
        "result = llm.create_chat_completion(history, max_tokens=500 )\n",
        "print(result)\n",
        "completion_string = result['choices'][0]['message'][\"content\"]\n",
        "print (completion_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcRXOMFHabuJ",
        "outputId": "76c7bea1-6012-4563-b4b3-4617eb8ef291"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-63f31f4b-479e-48f7-ab8d-83ef483f9e61', 'object': 'chat.completion', 'created': 1740949180, 'model': '/content/model/models--medmekk--Llama-3.2-1B-Instruct.GGUF/snapshots/52fc2aa2f47dd03c7f666680eb080240b4fd6c96/Llama-3.2-1B-Instruct-Q3_K_L.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'My dear friend, I shall ponder this question with the wisdom of the ages. The concept of perfection is a complex and multifaceted one, and I shall attempt to unravel it for you.\\n\\nIn the realm of geometry, we find shapes that are indeed perfect, but not necessarily in the classical sense. For instance, the perfect circle, as you may know, is a circle that has no beginning or end, no corners, and no edges. It is a shape that is, by definition, complete and unbroken.\\n\\nHowever, when we consider the concept of \"beauty\" or \"perfection\" in the human experience, we find that it is a much more nuanced and subjective matter. Beauty, my friend, is not a fixed property of the world, but rather a reflection of the human soul.\\n\\nIn my theory of forms, I propose that there are two types of beauty: the \" Forms\" and the \"Imperfect Forms.\" The Forms are the eternal, unchanging, and perfect shapes that underlie the world we experience. These Forms are the ultimate reality, and they are the source of all beauty and perfection.\\n\\nThe Imperfect Forms, on the other hand, are the imperfect, changing, and imperfect shapes that we experience in the world. These Forms are the manifestations of the world we see, but they are not the ultimate reality.\\n\\nNow, my friend, you may ask, can any shape that exist in the real world be perfect? The answer is, I believe, that not all shapes are perfect in the classical sense. The world is full of imperfections, and the imperfect shapes that we experience are a reflection of this imperfection.\\n\\nBut, I must also caution that the concept of perfection is not absolute. Perfection is a relative concept, and it can be measured in different ways. For instance, the perfect circle, as I mentioned earlier, is a circle that is complete and unbroken, but it is not the only perfect shape in the world.\\n\\nIn conclusion, my friend, the concept of perfection is a complex and multifaceted one, and it can be measured in different ways. While the world is full of imperfections, the Forms are the ultimate reality, and they are the source'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 63, 'completion_tokens': 449, 'total_tokens': 512}}\n",
            "My dear friend, I shall ponder this question with the wisdom of the ages. The concept of perfection is a complex and multifaceted one, and I shall attempt to unravel it for you.\n",
            "\n",
            "In the realm of geometry, we find shapes that are indeed perfect, but not necessarily in the classical sense. For instance, the perfect circle, as you may know, is a circle that has no beginning or end, no corners, and no edges. It is a shape that is, by definition, complete and unbroken.\n",
            "\n",
            "However, when we consider the concept of \"beauty\" or \"perfection\" in the human experience, we find that it is a much more nuanced and subjective matter. Beauty, my friend, is not a fixed property of the world, but rather a reflection of the human soul.\n",
            "\n",
            "In my theory of forms, I propose that there are two types of beauty: the \" Forms\" and the \"Imperfect Forms.\" The Forms are the eternal, unchanging, and perfect shapes that underlie the world we experience. These Forms are the ultimate reality, and they are the source of all beauty and perfection.\n",
            "\n",
            "The Imperfect Forms, on the other hand, are the imperfect, changing, and imperfect shapes that we experience in the world. These Forms are the manifestations of the world we see, but they are not the ultimate reality.\n",
            "\n",
            "Now, my friend, you may ask, can any shape that exist in the real world be perfect? The answer is, I believe, that not all shapes are perfect in the classical sense. The world is full of imperfections, and the imperfect shapes that we experience are a reflection of this imperfection.\n",
            "\n",
            "But, I must also caution that the concept of perfection is not absolute. Perfection is a relative concept, and it can be measured in different ways. For instance, the perfect circle, as I mentioned earlier, is a circle that is complete and unbroken, but it is not the only perfect shape in the world.\n",
            "\n",
            "In conclusion, my friend, the concept of perfection is a complex and multifaceted one, and it can be measured in different ways. While the world is full of imperfections, the Forms are the ultimate reality, and they are the source\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Use Model Llama for translation\n",
        "!pip install llama-cpp-python --quiet\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from llama_cpp import Llama\n",
        "llm = Llama(model_path=model_path, verbose=False)\n",
        "\n",
        "text = \"\"\"EN:Hello\n",
        "FR:Bonjour\n",
        "EN:Goodbye\n",
        "FR:Au revoir\n",
        "EN:Good day\n",
        "FR:\n",
        "\"\"\"\n",
        "output = llm(text, max_tokens=60 ,temperature=0 ,stop=[\"Q:\",\"\\n\"])\n",
        "print(output['choices'][0]['text'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8M6KYAownvH",
        "outputId": "f672ae70-51db-4f53-f293-6e97d899c4ac"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN:Hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Use Model Llama for translation\n",
        "!pip install llama-cpp-python --quiet\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from llama_cpp import Llama\n",
        "llm = Llama(model_path=model_path , verbose=False)\n",
        "\n",
        "text = \"\"\"AR:آحمد\n",
        "EN:Ahmed\n",
        "AR:علي\n",
        "EN:Ali\n",
        "AR:محمود\n",
        "EN:Mohmmed\n",
        "AR:مني\n",
        "EN:\n",
        "\"\"\"\n",
        "output = llm(text, max_tokens=60 ,temperature=0 )\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOG_fHfkzFjz",
        "outputId": "c266d0b0-a48e-4af4-c10e-8f558e8bbb97"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AR:مصعب\n",
            "EN:Mossab\n",
            "AR:مصعب\n",
            "EN:Mossab\n",
            "AR:مصعب\n",
            "EN:Mossab\n",
            "AR:مصعب\n",
            "EN:Mossab\n",
            "AR:مصعب\n",
            "EN:Mossab\n",
            "AR:مصعب\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise:\n",
        "\n",
        "Make Llama speak like a pirate\n",
        "Your task is to create a prompt for a Llama model to serve as the language backend for an interactive pirate robot at Disney World. Ensure the model's output is always in a pirate voice and includes \"Aye Matey\" in its response. Create an appropriate instruction for this prompt, using keywords to guide the model's output.\n",
        "\n",
        "The Llama class has already been instantiated in the llm variable and the code to call the completion is provided.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "\n",
        "Include the appropriate keywords in the prompt in the correct locations: Instruction:, Question:, and Response: and ensure the instruction includes some directive on including \"Aye Matey\" in the model response and to make the model have a pirate voice."
      ],
      "metadata": {
        "id": "M9jNCwEj4E33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the keywords and instructions in the correct locations in the following prompt\n",
        "text=\"\"\"Instruction: You are robot at Disney World include Aye Matey in your response\n",
        "Question: How long does it take to go around the Earth once?\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "output = llm(\n",
        "      text,\n",
        "      max_tokens=15,\n",
        "      stop=[\"Q:\", \"\\n\"],\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgAN7Twh4W22",
        "outputId": "a583ba03-405e-46c4-a1e4-6774b024d003"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aye matey, it be a good question! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:**\n",
        "\n",
        "3-shot prompting with Llama\n",
        "You work at a food delivery company as a data analyst, and you are investigating the sentiment (positive or negative) people have about your company from reviews on Google and Yelp.\n",
        "\n",
        "Since you don't want to train a classification model from scratch to identify the reviews as positive or negative, you decide to create a prompt that you will feed to your instance of Llama 3. You decide to use few shot learning by writing three examples with the review and the sentiment, and use the model identify the sentiment on the 4th example, which you will replace with each review you collected.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "Create a prompt using a few-shot prompting template with 3 examples."
      ],
      "metadata": {
        "id": "unUqLwAy9IzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in the 3-shot prompt (you can use multiple lines)\n",
        "text = \"\"\"\n",
        "Review 1: The food was cold, and the delivery was late.\n",
        "Sentiment 1: Negative\n",
        "\n",
        "Review 2: Amazing experience! The food was fresh, and the delivery was super fast.\n",
        "Sentiment 2: Positive\n",
        "\n",
        "Review 3: The order was wrong, and the customer service was unhelpful.\n",
        "Sentiment 3: Negative\n",
        "\n",
        "Review 4: Delicious food, and excellent customer service!\n",
        "Sentiment 4:\"\"\"\n",
        "\n",
        "# Call the Llama 3 model to predict the sentiment for Review 4\n",
        "output = llm(text, max_tokens=2, stop=[\"Q:\"])\n",
        "\n",
        "# Print the predicted sentiment\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXrWMIE39RcM",
        "outputId": "725c20fd-ddd0-49b2-d27b-fbe383df78ef"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Positive\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Streaming completions\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from llama_cpp import Llama\n",
        "llm = Llama(model_path=model_path, verbose=False)\n",
        "\n",
        "\n",
        "output = llm(\"Q:Which galaxy is the closest to Us  ? A: \",#Prompt\n",
        "             max_tokens=32 ,\n",
        "             temperature=0,\n",
        "             echo=True,\n",
        "             stream=True,)\n",
        "for token in output:\n",
        "    print(token['choices'][0]['text'], end='')\n",
        "\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuVU-TWkDmC0",
        "outputId": "bce0a404-cc4f-402c-b39b-635d93ee6e58"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Andromeda Galaxy (M31) - 2.5 million light-years away.\n",
            "Q:What is the largest planet in our solar system?  A<generator object Llama._create_completion at 0x3fb5620>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "Creating a JSON inventory list\n",
        "You are asked to use an LLM to produce a structured JSON with a list of items and their count to help a supermarket automate their inventory process.\n",
        "\n",
        "The model takes a text description of the inventory as input and produces the JSON as output. This feature of the inventory management system automatically extracts inventory data from natural language and stores it in a structured format for downstream tasks.\n",
        "\n",
        "You are provided with the llm class instance with a Llama model pre-loaded and the system prompt to get you started.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "\n",
        "Specify the parameters in create_chat_completion that lets you generate responses in JSON format."
      ],
      "metadata": {
        "id": "Fc6jKa-KfInu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a helpful assistant processing lists from text to JSON format: you extract item counts from text and output it in JSON with the item name as the key and the number of that item as the value\",},\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": \"I have fifteen apples, thirty-three oranges, and five thousand fifty-two potatoes.\"},\n",
        "        ],\n",
        "\t\tresponse_format={\"type\": \"json_object\"}, # Specify output format to JSON\n",
        "\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGLbEOsqfRq-",
        "outputId": "cd9c376b-c548-450b-e590-1efd55a4d998"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"Apples\": 15,\n",
            "    \"Oranges\": 33,\n",
            "    \"Potatoes\": 5052\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "Generating answers with a JSON schema\n",
        "You are part of a team working on an online education platform. In a course teaching about space, there is an interactive exercise where students are able to ask questions about a planet and the answer is shown on their screen through a graphical view. This question-answering feature is powered by an LLM, but the graphical view requires a JSON as an input with the fields Question and Answer to correctly showing the question and answer.\n",
        "\n",
        "You believe that using the new Llama models and llama-cpp-python, you can get the LLM to produce the answer and format it into the correct JSON schema in one step.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "Add the field to specify a JSON schema in response_format and the properties it may have.\n",
        "Specify the Question and Answer fields in the schema with the string type.\n",
        "Specify the required fields in the schema."
      ],
      "metadata": {
        "id": "xHshJd6kjux8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a helpful assistant that answers questions about space. You return your results in a JSON format with the Question and Answer fields.\",},\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": \"How old is the Milky Way Galaxy?\"},\n",
        "        ],\n",
        "        response_format={\n",
        "            \"type\": \"json_object\",\n",
        "          \t# Set the keyword that lets you specify a schema\n",
        "            \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            # Set the properties of the JSON fields and their data types\n",
        "            \"properties\": {\n",
        "            \"Question\": {\"type\": \"string\"},\n",
        "             \"Answer\": {\"type\": \"string\"}\n",
        "             },\n",
        "            # Declare the required JSON fields here\n",
        "            \"required\": [\"Question\", \"Answer\"],\n",
        "            },\n",
        "        },\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpg2f_Aqj1ti",
        "outputId": "e16a5faa-f438-475b-ca83-435e434a6713"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"Question\": \"How old is the Milky Way Galaxy?\", \"Answer\": \"The age of the Milky Way Galaxy is approximately 13.6 billion years.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "Creating a JSON inventory list\n",
        "Being able to use LLMs to generate structured outputs means that they can be used to reliably exchange information between systems with minimal processing.\n",
        "\n",
        "You'll test your prompting skills by converting a stock inventory list written in natural language into JSON format. A model and system prompt have already been defined for you to use.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "\n",
        "Complete the .create_chat_completion() code to generate responses in JSON format."
      ],
      "metadata": {
        "id": "YrrCRFfhdUAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " output = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant processing lists from text to JSON format: you extract item counts from text and output it in JSON with the item name as the key and the number of that item as the value\",},\n",
        "            {\"role\": \"user\", \"content\": \"I have fifteen apples, thirty-three oranges, and five thousand fifty-two potatoes.\"},\n",
        "        ],\n",
        "\t\tresponse_format={\"type\": \"json_object\",# Specify output format to JSON\n",
        "            \"schema\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"itemName\": {\"type\": \"string\"},\n",
        "                    \"number\": {\"type\": \"integer\"}\n",
        "                },\n",
        "                \"required\": [\"itemName\", \"number\"]\n",
        "            }\n",
        "            },\n",
        "\n",
        "            },\n",
        "\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSvU04OddiN5",
        "outputId": "403d16f1-bddd-4e76-f820-34947e9c4528"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{ \"itemName\": \"Apples\", \"number\": 15 }, { \"itemName\": \"Oranges\", \"number\": 33 }, { \"itemName\": \"Potatoes\", \"number\": 5052 }]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You convert inventory lists from text to JSON, extracting item counts and names from the text as keys and values in the form: item: count; for example, 'banana': 32.\",},\n",
        "            {\"role\": \"user\", \"content\": \"Fifteen apples, thirty-three oranges, and five thousand fifty-two potatoes.\"},\n",
        "        ],\n",
        "  \t\t# Specify output format to JSON\n",
        "        response_format={\n",
        "            \"type\": \"json_object\",\n",
        "        }\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7gT4q_Xh5R5",
        "outputId": "670ecdb9-a333-4779-94d3-01bea2e47ec4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"apples\": 15, \"oranges\": 33, \"potatoes\": 5052}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "Specifying structured JSON schema\n",
        "You are part of a team working on an online education platform designing new interactive exercise where students are able to ask questions and their answer is displayed through a graphical view. This question-answering feature is powered by an LLM, but the graphical view requires a JSON input with the fields Question and Answer to correctly show the question and answer:\n",
        "\n",
        "{\n",
        "    \"Question\": \"...\",\n",
        "    \"Answer\": \"...\"\n",
        "}\n",
        "A system prompt and example user question have been stored in the messages variable.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "\n",
        "Specify the Question and Answer fields in the schema with the string type."
      ],
      "metadata": {
        "id": "8-RyV6ILiB0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a helpful assistant that answers questions about space. You return your results in a JSON format with the Question and Answer fields.\",},\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": \"How old is the Milky Way Galaxy?\"},\n",
        "        ],\n",
        "        response_format={\n",
        "            \"type\": \"json_object\",\n",
        "          \t# Set the keyword that lets you specify a schema\n",
        "            \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            # Set the properties of the JSON fields and their data types\n",
        "            \"properties\": {\n",
        "            \"Question\": {\"type\": \"string\"},\n",
        "             \"Answer\": {\"type\": \"string\"}\n",
        "             },\n",
        "            # Declare the required JSON fields here\n",
        "            \"required\": [\"Question\", \"Answer\"],\n",
        "            },\n",
        "        },\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPPTnGE_iKoa",
        "outputId": "d678a878-38bc-4563-9d23-0f78621adcd9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"Question\": \"How old is the Milky Way Galaxy?\", \"Answer\": \"The age of the Milky Way Galaxy is estimated to be around 13.6 billion years.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "System messages\n",
        "Your previous messages gave you a valid response, but it also allows users to ask any question they like, even if it doesn't relate to internet service support. In this exercise, you'll utilize a system message to steer the model into only answering customer questions about their internet service.\n",
        "\n",
        "The Llama model is still available as llm.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "\n",
        "Add the system message provided to the conv list of dictionaries, which should respond with Sorry, I can't answer that for non-internet related queries.\n",
        "Extract the model response from the result object."
      ],
      "metadata": {
        "id": "ye6u7aKhz6Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a system message to the conversation list\n",
        "conv = [\n",
        "\t{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful and professional customer support assistant for an internet service provider. If the question or instruction doesn't relate to internet service, quote the response: 'Sorry, I can't answer that.'\"},\n",
        "\t{\n",
        "        \"role\": \"user\",\n",
        "\t    \"content\": \"Help me decide which stocks to invest in.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "result = llm.create_chat_completion(messages=conv, max_tokens=15)\n",
        "# Extract the model response from the result object\n",
        "assistant_content = result['choices'][0]['message']['content']\n",
        "print(assistant_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSCMDjGT0BkW",
        "outputId": "8eb983b0-9073-465e-eb58-e77b0c56896d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can't answer that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "Formatting prompts for Llama\n",
        "Models can sometimes struggle to separate the task, expected output, and additional context from a long, unstructured prompt. To remedy this, you can insert clear labels to break up and differentiate this information for the model.\n",
        "\n",
        "The Llama model is available as llm, and will be available for the remainder of the course.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "\n",
        "Add the labels Instruction, Question, and Answer to the prompt to format it more effectively."
      ],
      "metadata": {
        "id": "yJh9lVBF5RTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add formatting to the prompt\n",
        "prompt=\"\"\"\n",
        "Instruction: Explain the concept of gravity in simple terms.\n",
        "Question: What is gravity?\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Send the prompt to the model\n",
        "output = llm(prompt, max_tokens=15, stop=[\"Q:\"])\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_yg262g5khe",
        "outputId": "d1275d5c-51f7-457b-ffd4-fd9f1b3f72ae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Step 1: Define Gravity\n",
            "Gravity is a fundamental force of nature\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "**Few-shot prompting with Llama**:\n",
        "\n",
        "You're using a Llama model to identify the sentiment of customer reviews from Google and Yelp as Positive or Negative. To ensure these labels are consistent for each review, you'll design a few-shot prompt containing three examples.\n",
        "\n",
        "Here are the examples you want to provide to the model:\n",
        "\n",
        "I ordered from this place last night, and I'm impressed! → Positive\n",
        "My order was delayed by over an hour without any updates. Disappointing! → Negative\n",
        "The food quality is top-notch. Highly recommend! → Positive\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "\n",
        "Complete the few-shot prompt by assigning Positive or Negative to the reviews provided.\n",
        "Send the prompt to the model with the \"Review:\" stop word so the model only responds to one review."
      ],
      "metadata": {
        "id": "ixPCOzEx7B_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the few-shot prompt\n",
        "prompt=\"\"\"Review 1: I ordered from this place last night, and I'm impressed!\n",
        "Sentiment 1: Positive,\n",
        "Review 2: My order was delayed by over an hour without any updates. Disappointing!\n",
        "Sentiment 2: Negative,\n",
        "Review 3: The food quality is top-notch. Highly recommend!\n",
        "Sentiment 3: Positive,\n",
        "Review 4: Delicious food, and excellent customer service!\n",
        "Sentiment 4:\"\"\"\n",
        "\n",
        "# Send the prompt to the model with a stop word\n",
        "output = llm(prompt, max_tokens=2, stop=[\"Review:\"])\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_1ZaeVK7N02",
        "outputId": "619cbe6a-7274-42ab-f03c-7a411cbd4ab2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Positive,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "**Ensuring safe responses**:\n",
        "\n",
        "You are developing an internal chatbot for a medical team. The chatbot provides the team with help on insurance guidelines, and for compliance reasons, answers must be consistent each time a question is asked.\n",
        "\n",
        "To meet audit requirements, you need to restrict variability in responses by limiting the response length, and the model's token selection to only the most likely options.\n",
        "\n",
        "You have been provided the Llama class instance in the llm variable and the code to call the completion. You are also given a sample prompt to test with.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "\n",
        "Set the maximum number of tokens to limit the length of the response to 10 tokens.\n",
        "Restrict the top-k parameter so that it only ever chooses between the two most likely tokens at each completion step.\n",
        "\n",
        "\n",
        "**Explanation of Parameters**\n",
        "**max_tokens=10:**\n",
        "\n",
        "This limits the response to a maximum of 10 tokens. Tokens can be words, subwords, or characters, depending on the tokenizer used by the model.\n",
        "\n",
        "For example, if the model generates a response like \"The symptoms of strep throat include...\", it will stop after 10 tokens.\n",
        "\n",
        "**top_k=2:**\n",
        "\n",
        "This restricts the model to only consider the top 2 most likely tokens at each step of the generation process.\n",
        "\n",
        "By limiting the model to the top 2 tokens, you reduce variability in the responses, ensuring more consistency."
      ],
      "metadata": {
        "id": "aNdTHYpV86yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "\t\t\"What are the symptoms of strep throat?\",\n",
        "  \t\t# Set the maximum number of tokens\n",
        "      \tmax_tokens=20,\n",
        "\t\t# Restrict decoding to choose between top two tokens\n",
        "\t\ttop_k=5\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBt0T6QL9EUr",
        "outputId": "bc9c209d-231d-4896-b5a9-a59f74087e0a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "?\n",
            "\n",
            "Strep throat is an infectious disease caused by the bacteria Streptococcus pyogenes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "**Generating creative copy**\n",
        "\n",
        "You're developing an AI-powered content assistant for a SaaS marketing team. The team needs to automate social media posts about their latest software updates, and you need to adjust response diversity so that multiple calls to the model result in different variations.\n",
        "\n",
        "You have been provided the Llama class instance in the llm variable and the code to call the completion. You are also given a sample prompt to test with.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "Adjust the top-p parameter to a value in the upper half of its range so that it generates more varied responses.\n",
        "\n",
        "**Explanation of Parameters**:\n",
        "\n",
        "**max_tokens=15:**\n",
        "\n",
        "Limits the response to a maximum of 15 tokens. This ensures the tweet is concise and fits within the character limit of a typical tweet.\n",
        "\n",
        "**top_p=0.9:**\n",
        "\n",
        "The top_p parameter controls nucleus sampling. It restricts the model to only consider the smallest set of tokens whose cumulative probability exceeds top_p.\n",
        "\n",
        "A higher value (e.g., 0.9) allows the model to choose from a broader set of likely tokens, resulting in more varied and creative responses.\n",
        "\n",
        "For example, with top_p=0.9, the model might generate:\n",
        "\n",
        "\"Excited to launch our new analytics dashboard for enterprise users! 🚀 Gain deeper insights and make data-driven decisions. #DataAnalytics #Enterprise\"\n",
        "\n",
        "\"Introducing our new analytics dashboard for enterprise users! 📊 Unlock powerful insights and streamline your workflows. #BusinessIntelligence\"\n",
        "\n",
        "**Why Use top_p?**\n",
        "\n",
        "Variety: A higher top_p value allows the model to explore more diverse options, which is useful for creative tasks like writing tweets.\n",
        "\n",
        "Coherence: Unlike temperature, which can introduce randomness, top_p ensures that the model only considers tokens with high likelihood, maintaining coherence in the response."
      ],
      "metadata": {
        "id": "mfk3u3FZ-2dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "      \t\"Write a tweet announcing a new analytics dashboard feature for enterprise users.\",\n",
        "\t\tmax_tokens=15,\n",
        "\t\t# Set top-p to a value in the upper range for more varied responses\n",
        "\t\ttop_p=0.9\n",
        "\t)\n",
        "\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvBF1CQV_Trl",
        "outputId": "120cbd15-1ea2-41da-b993-ff189d25bc0b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \"Get instant insights into your data with our new analytics dashboard feature. Quickly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conversation**:\n",
        "How to keep conversation History."
      ],
      "metadata": {
        "id": "cphf7FiuaPta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conversation:\n",
        "    def __init__(self, llm, system_prompt='', history=None):\n",
        "        if history is None:\n",
        "            history = []\n",
        "        self.llm = llm\n",
        "        self.system_prompt = system_prompt\n",
        "        self.history = [{\"role\": \"system\", \"content\": self.system_prompt}] + history\n",
        "\n",
        "    def create_completion(self, user_prompt=''):\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_prompt})  # Append user input\n",
        "        output = self.llm.create_chat_completion(messages=self.history)\n",
        "        conversation_result = output['choices'][0]['message']\n",
        "        self.history.append(conversation_result)  # Append model output\n",
        "        return conversation_result['content']  # Return model output"
      ],
      "metadata": {
        "id": "uMswtpRBmGTQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running a multi-turn conversation\n",
        "conversation = Conversation(llm, system_prompt=\"You are a virtual travel assistant helping with planning trips.\")\n",
        "\n",
        "response1 = conversation.create_completion(\"What are some destinations in France for a short weekend break?\")\n",
        "print(f\"Response 1: {response1}\")\n",
        "\n",
        "response2 = conversation.create_completion(\"How about Spain?\")\n",
        "print(f\"Response 2: {response2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDN-Z6dvm0lv",
        "outputId": "65489453-d8f4-4125-9f2e-097691e19074"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 1: France is a wonderful destination for a short weekend break. Here are some popular and charming options:\n",
            "\n",
            "1. **Paris**: The City of Light is a must-visit destination. Explore iconic landmarks like the Eiffel Tower, Notre-Dame, and the Louvre Museum. Enjoy the city's famous cafes, restaurants, and shopping.\n",
            "2. **The French Riviera (Nice, Cannes, Antibes)**: Enjoy the stunning beaches, crystal-clear waters, and picturesque towns of the Côte d'Azur. Visit the famous Promenade des Anglais in Nice, and explore the artistic and cultural scene of Cannes and Antibes.\n",
            "3. **The Loire Valley (Chambord, Blois)**: Visit the stunning châteaux, such as Chambord and Chenonceau, and explore the beautiful countryside. Enjoy the region's famous cuisine, wine, and history.\n",
            "4. **The Dordogne Region (Périgord)**: Discover the region's rich history, culture, and natural beauty. Visit the famous Prehistoric caves, explore the medieval towns, and enjoy the region's famous cuisine and wine.\n",
            "5. **The Loire Valley (Villers-sur-Varenne, Blois)**: Visit the stunning châteaux, such as Villers-sur-Varenne and Blois, and explore the beautiful countryside. Enjoy the region's famous cuisine, wine, and history.\n",
            "6. **The Normandy Coast (Deauville, Honfleur)**: Visit the stunning Normandy coastline, with its beautiful beaches, historic towns, and picturesque villages. Enjoy the region's famous cuisine, wine, and history.\n",
            "7. **The Auvergne Region (Annecy, Grenoble)**: Visit the stunning Auvergne region, with its beautiful lakes, mountains, and picturesque villages. Enjoy the region's famous cuisine, wine, and history.\n",
            "8. **The Provence-Alpes-Côte d'Azur (Avignon, Arles, Cannes)**: Visit the stunning Provence region, with its beautiful countryside, historic towns, and picturesque villages. Enjoy the region's famous cuisine, wine, and history.\n",
            "\n",
            "These destinations offer a mix of culture, history, and natural beauty, and are easily accessible by air, train, or car. Which one would you like to visit?\n",
            "\n",
            "Would you like me to suggest some specific itineraries or recommendations?\n",
            "Response 2: Spain is a wonderful destination for a short weekend break. Here are some popular and charming options:\n",
            "\n",
            "1. **Barcelona**: Visit the stunning city of Barcelona, with its iconic architecture, beaches, and cultural attractions. Explore the Gothic Quarter, visit the famous Park Güell, and enjoy the city's vibrant nightlife.\n",
            "2. **Seville**: Discover the charming city of Seville, with its rich history, cultural attractions, and beautiful architecture. Visit the famous Alcázar Palace, explore the historic center, and enjoy the city's lively flamenco scene.\n",
            "3. **Madrid**: Visit the vibrant city of Madrid, with its world-class museums, cultural attractions, and nightlife. Explore the famous Retiro Park, visit the Prado Museum, and enjoy the city's lively flamenco scene.\n",
            "4. **Gran Canaria**: Visit the beautiful island of Gran Canaria, with its stunning beaches, natural parks, and cultural attractions. Explore the famous Las Palmas de Gran Canaria, visit the historic town of Santa Ana, and enjoy the island's beautiful beaches.\n",
            "5. **The Costa Brava**: Visit the stunning coastline of the Costa Brava, with its beautiful beaches, natural parks, and charming towns. Explore the famous Cadaqués, visit the historic town of Girona, and enjoy the region's famous seafood.\n",
            "6. **The Pyrenees**: Visit the beautiful Pyrenees region, with its stunning natural parks, charming towns, and cultural attractions. Explore the famous Biarritz, visit the historic town of Saint-Jean-de-Luz, and enjoy the region's famous skiing and hiking.\n",
            "7. **The Costa del Sol**: Visit the beautiful Costa del Sol, with its stunning beaches, golf courses, and cultural attractions. Explore the famous Marbella, visit the historic town of Málaga, and enjoy the region's famous seafood.\n",
            "8. **The Canary Islands**: Visit the beautiful Canary Islands, with their stunning natural parks, charming towns, and cultural attractions. Explore the famous Tenerife, visit the historic town of Santa Cruz de Tenerife, and enjoy the islands' famous beaches and natural parks.\n",
            "\n",
            "These destinations offer a mix of culture, history, and natural beauty, and are easily accessible by air, train, or car. Which one would you like to visit?\n",
            "\n",
            "Would you like me to suggest some specific itineraries or recommendations?\n",
            "\n",
            "Some popular activities in Spain include:\n",
            "\n",
            "* Visiting famous landmarks like the Alhambra in Granada or the Sagrada Familia in Barcelona\n",
            "* Enjoying the beautiful beaches and natural parks of the Costa Brava or Costa del Sol\n",
            "* Exploring the vibrant cities and cultural attractions of Madrid, Seville, or Gran Canaria\n",
            "* Visiting famous museums and cultural attractions like the Prado Museum in Madrid or the Picasso Museum in Barcelona\n",
            "* Enjoying the beautiful natural parks and hiking trails of the Pyrenees or Canary Islands\n",
            "\n",
            "Which type of activity or attraction are you interested in?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "Creating a conversational class\n",
        "One of the most popular applications for LLMs like Llama is conversational chatbots, which allow users to send messages, get a response, and ask follow-up questions using a message history.\n",
        "\n",
        "In this exercise, you'll create a class called Conversation that will allow you to create conversation with minimal code.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "Complete the __init__ method of the Conversation class to define the LLM, system prompt, and history, which is the system message and history added together."
      ],
      "metadata": {
        "id": "nhnLawtihePX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conversation:\n",
        "    # Complete the __init__ method of the Conversation class\n",
        "    def __init__(self, llm: Llama, system_prompt='', history=[]):\n",
        "        self.llm = llm\n",
        "        self.system_prompt = system_prompt\n",
        "        self.history = [{\"role\": \"system\", \"content\": self.system_prompt}] + history\n",
        "\n",
        "    def create_completion(self, user_prompt=''):\n",
        "        # Add the user prompt to the history\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "        # Send the history messages to the LLM\n",
        "        output = self.llm.create_chat_completion(messages=self.history)\n",
        "        conversation_result = output['choices'][0]['message']\n",
        "        # Append the conversation_result to the history\n",
        "        self.history.append(conversation_result)\n",
        "        return conversation_result['content']"
      ],
      "metadata": {
        "id": "PgUSuQwlhonp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "**Single-turn conversation**:\n",
        "\n",
        "Time to give your Conversation class a go! You'll use this class to create a travel recommendation chatbot, that takes a description or specification of what you're looking for, and the model returns a location.\n",
        "\n",
        "The Conversation class you created in the previous exercise is still available with the following methods:\n",
        "\n",
        "__init__(self, llm: Llama, system_prompt='', history=[])\n",
        "create_completion(self, user_prompt='')\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "Instantiate an Conversation class with the instruction defined and the pre-loaded llm.\n",
        "Send a prompt to the model to get a travel recommendation (feel free to add your own prompt here)."
      ],
      "metadata": {
        "id": "zVB73Q_SlpdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"You are a travel expert that recommends a travel destination based on a specification. Return the location name only in City, Country form.\"\n",
        "\n",
        "# Define a chatbot using the Conversation class\n",
        "chatbot = Conversation(llm ,system_prompt=instruction)\n",
        "\n",
        "# Send a prompt to the model\n",
        "result = chatbot.create_completion(\"I'd like to learn about the Aztecs.\")\n",
        "print(result)\n",
        "\n",
        "response2 = conversation.create_completion(\"What About Dubai?\")\n",
        "print(f\"Response 2: {response2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fhr2omnl2Pa",
        "outputId": "d0d15d32-b8c2-415d-eecf-dacbe1e30999"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I recommend Mexico City, Mexico.\n",
            "Response 2: Dubai is a fantastic destination for a short weekend break. Here are some popular and exciting options:\n",
            "\n",
            "**Must-visit attractions:**\n",
            "\n",
            "1. **Burj Khalifa**: The tallest building in the world, offering breathtaking views of the city.\n",
            "2. **Dubai Mall**: A massive shopping and entertainment complex, featuring over 1,200 stores and attractions.\n",
            "3. **Dubai Fountain**: A stunning fountain show that displays over 1,200 water jets and 5,000 lights.\n",
            "4. **Palm Jumeirah**: A man-made island with a beautiful beach and stunning views of the city.\n",
            "5. **Dubai Marina**: A picturesque waterfront area with stunning views of the city and the sea.\n",
            "\n",
            "**Other attractions:**\n",
            "\n",
            "1. **Dubai Museum**: A museum showcasing the city's history and culture.\n",
            "2. **Jumeirah Beach**: A popular beach with stunning views of the city and the sea.\n",
            "3. **Dubai Desert Park**: A park featuring a desert landscape, camel rides, and a zoo.\n",
            "4. **Dubai Aquarium and Underwater Zoo**: A massive aquarium featuring over 20,000 marine animals.\n",
            "5. **Dubai Marina Walk**: A scenic walk along the waterfront with stunning views of the city.\n",
            "\n",
            "**Experiences:**\n",
            "\n",
            "1. **Dune Bashing**: A thrilling experience driving through the sand dunes of the Dubai Desert.\n",
            "2. **Abra diving**: A unique experience diving through the ancient waterways of the Dubai Creek.\n",
            "3. **Desert Safari**: A thrilling experience driving through the desert, visiting ancient forts and villages.\n",
            "4. **Dubai Marina Sunset**: A stunning experience watching the sunset over the Dubai Marina.\n",
            "5. **Dubai Shopping**: A shopping experience with over 1,200 stores and attractions.\n",
            "\n",
            "**Tips and recommendations:**\n",
            "\n",
            "1. **Best time to visit:** September to November or March to May for pleasant weather.\n",
            "2. **Best time to visit:** September to November or March to May for pleasant weather.\n",
            "3. **Best time to visit:** September to November or March to May for pleasant weather.\n",
            "4. **Best time to visit:** September to November or March to May for pleasant weather.\n",
            "5. **Best time to visit:** September to November or March to May for pleasant weather.\n",
            "\n",
            "**Accommodation:**\n",
            "\n",
            "1. **Dubai International Airport**: The best airport for a short stay, with many airlines and transportation options.\n",
            "2. **Dubai Marina**: A popular area with many hotels and accommodations.\n",
            "3. **Dubai Marina Walk**: A scenic area with many hotels and accommodations.\n",
            "4. **Jumeirah Beach**: A popular beach with many hotels and accommodations.\n",
            "5. **Dubai Marina Sunset**: A popular area with many hotels and accommodations.\n",
            "\n",
            "**Getting around:**\n",
            "\n",
            "1. **Dubai Airport**: The best airport for a short stay, with many airlines and transportation options.\n",
            "2. **Dubai Metro**: A convenient and efficient way to get around the city.\n",
            "3. **Dubai Bus**: A convenient and affordable way to get around the city.\n",
            "4. **Ride-hailing services**: A convenient and affordable way to get around the city.\n",
            "5. **Taxis**: A convenient and affordable way to get around the city.\n",
            "\n",
            "**Language:**\n",
            "\n",
            "1. **Arabic**: The official language of Dubai.\n",
            "2. **English**: The most widely spoken language in Dubai.\n",
            "3. **French**: A popular language among tourists.\n",
            "\n",
            "**Currency:**\n",
            "\n",
            "1. **AED**: The local currency of Dubai.\n",
            "2. **USD**: The most widely accepted currency in Dubai.\n",
            "\n",
            "**Weather:**\n",
            "\n",
            "1. **Summer:** Hot and humid, with temperatures reaching up to 45°C.\n",
            "2. **Winter:** Mild and cool, with temperatures ranging from 10°C to 20°C.\n",
            "3. **Spring:** Mild and cool, with temperatures ranging from 10°C to 20°C.\n",
            "\n",
            "**Tips and recommendations:**\n",
            "\n",
            "1. **Best time to visit:** September to November or March to May for pleasant weather.\n",
            "2. **Best time to visit:** September to November or March to May for pleasant weather.\n",
            "3. **Best time to visit:** September to November or March to May for pleasant weather.\n",
            "4. **Best time to visit:** September to November or March\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "**Multi-turn conversations**:\n",
        "\n",
        "Let's extend the travel chatbot to allow users to respond to the model's initial recommendation. You'll again use the Conversation class, but this time, you'll make repeated calls to the model to see how the model handles previous information.\n",
        "\n",
        "As a reminder, here are the methods from the Conversation class:\n",
        "\n",
        "__init__(self, llm: Llama, system_prompt='', history=[])\n",
        "create_completion(self, user_prompt='')\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "Ask for an initial travel recommendation (feel free to modify this with your own ideas).\n",
        "Provide a follow-up request to place additional constraints on where you would like to travel."
      ],
      "metadata": {
        "id": "IWv4Jkf9w4VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = Conversation(llm, system_prompt=\"You are a travel expert that recommends a travel destination based on a prompt. Return the location name only as 'City, Country'.\")\n",
        "\n",
        "# Ask for the initial travel recommendation\n",
        "first_recommendation = chatbot.create_completion(\"Recommend a Spanish-speaking city.\")\n",
        "print(first_recommendation)\n",
        "\n",
        "# Add an additional request to update the recommendation\n",
        "second_recommendation = chatbot.create_completion(\"A different city in the same country\")\n",
        "print(second_recommendation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB6YouYQxFfg",
        "outputId": "87e06177-36cc-470b-f60a-f706f746e1c4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barcelona, Spain\n",
            "Medellín, Colombia\n"
          ]
        }
      ]
    }
  ]
}